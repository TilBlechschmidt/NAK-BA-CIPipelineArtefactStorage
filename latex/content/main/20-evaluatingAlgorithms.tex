\section{Evaluating algorithms}\label{sec:evaluation}
    The previous section defined and collected data from many data sources and used it to feed a purpose-built simulation. We will now be using this simulation to evaluate the relative performance of several algorithms.
    
    The algorithms in question will be grouped into three categories: Static algorithms, layered algorithms, and scoring based algorithms. The categories will be explained in more detail in each of the following subsections. %\todo{Find out if these definitions mean something in existing literature}
    
    It should be noted that all graphs plotting the performance over time of algorithms are using the same value range to make comparison easier. The X-Axis of the plots shows the number of processed simulation events. The logarithmic Y-Axis depicts the percentage of missed accesses (note that this is in relation to the cumulative number of accesses up to that X coordinate and not the total number of accesses). Since the algorithms will cover a vast range of performances, the figures will be using a logarithmic scale\footnote{For this reason, the range is going from 0,5\% to 100\%.} on the Y-Axis. The real-world data source uses a disk size of approximately \SI{800}{\giga\byte}. Since we are discarding about 22,5\% of pipelines and only approximate the size, which is expected to yield an overall lower influx, we will be using the first power of two that is smaller than $22,5\% * \SI{800}{\giga\byte} = \SI{620}{\giga\byte}$ for all initial simulations and comparisons. In section \ref{sec:overall}, we will be looking at the behaviour of the algorithms with different disk sizes.
    
    \input{content/main/evaluatingAlgorithms/static}
    \input{content/main/evaluatingAlgorithms/layered}
    \input{content/main/evaluatingAlgorithms/scoring}

    \subsection{Overall results \& Discussion}\label{sec:overall}
        In the previous sections, we looked at three different categories of algorithms and their respective performances. However, so far, we only considered the relative performance over the course of the simulation at a fixed virtual disk size. To compare all algorithms, we will be using a variable disk size and abstract the data captured during the simulation by only taking the final performance at the end into account. The simulated disk sizes will be calculated using powers of two with the size-ramp batch mode of the simulation. All the resulting data points are plotted in a heatmap shown in figure \ref{fig:size-heatmap}. The X-Axis is sorted by performances at 16GB, and the colour range depicts the access miss ratio with 100\%, indicating that no access could be fulfilled during the simulation.
        
        \colfig{image/graphs/size-ramp-heatmap}{Algorithm performances by disk size}{fig:size-heatmap}
        
        As previously observed in section \ref{sec:static} and \ref{sec:layered}, LIFO, SF, and LF perform particularly bad at the previously observed \SI{512}{\giga\byte} disk size. However, this also holds for all other disk sizes where these three algorithms exhibit the worst performance with access miss ratios of over 50\% across the chart. An additional remark is the behaviour of LIFO at disk sizes smaller than \SI{128}{\giga\byte}. It can be described best as a similar phenomenon to BÃ©lady's Anomaly, which is commonly known from CPU memory paging. The anomaly describes a situation where an increase in available resources, in this case, disk space, decreases an algorithm's performance even though it would be expected to perform better \cite{belady-anomaly}. When increasing the available disk space from \SI{32}{\giga\byte} to either \SI{64}{\giga\byte} or \SI{128}{\giga\byte}, the algorithm's performance drops by up to two per cent.
        
        Next up is our static control algorithm RAND. It outperformed the previously mentioned algorithms and especially at larger disk sizes managed to undercut MRU as well. LRU stays ahead at higher disk pressures but falls behind in relative performance as disk space increases. This indicates that, at least with the current implementation, it does not make sense to employ any of the aforementioned algorithms as a purely random approach requires fewer inputs, operates faster\footnote{As it requires less inputs, fewer queries to external data sources like databases or file systems are required thus the decision time is expected to be shorter.}, and yields better results. Requiring fewer inputs also comes with the advantage that it allows for a more straightforward decision-making pipeline, simplifying setup procedures in a new environment.
        
        At the far right, a group of similarly performing algorithms developed. FIFO, STATUS, MERGED and SCORE are all performing exceptionally well with less than 5\% of requests failing at 512GB. Of the four algorithms, STATUS performs the best with MERGED and FIFO tying and SCORE being slightly behind. It is surprising that one of the arguably simplest algorithms, FIFO, managed to tie with more complex, domain-driven algorithms. Depending on the application scenario, it might be favourable to use it over one of the other ones, as it does not require any external inputs and can treat the stored artefacts as a black box. However, even direct inputs like a pipeline's status improved its performance noticeably across the different disk sizes. As artefacts are usually pushed into the storage system from within the CI pipeline, the status is known up-front, and thus it might very well be worth the effort.\\ % \todo{Argument with a few more numbers like "1/x the disk size possible with same performance"}
        \\\label{sec:machine-learning}
        Another noteworthy observation is the potential for automated parameter discovery. In section \ref{sec:lru-relevancy-algorithm} it was discovered that a significant number of pipelines are not accessed at all and that perhaps an algorithm to decide whether or not a pipeline is relevant in general might be useful. While this is strictly speaking a subset of the problem we are trying to solve in general, the abstraction might allow for a more precise algorithm. Finding an answer to this problem would effectively split the problem into two. The first half of the decision-making process would decide whether or not the data is relevant, and the second half decides on how long to store it. When considering this, it might be possible to convert the problem into a machine learning classification challenge. Initial tests with data taken from the simulation database yielded over 78\% in precision and recall for a model trained with a black box machine learning tool using a boosted tree algorithm. This has been purely experimental without any considerations taken regarding to over-fitting or other potential pitfalls in machine learning. It serves as a proof of concept showing that this approach bears potential. While the author has neither the expertise nor space required to elaborate and evaluate in full, it is an intriguing entry point for future research.
        
        Speaking of machine learning: During our evaluation of the scoring based approach in section \ref{sec:scoring}, we encountered issues with the parameter space growing too large to choose well-optimised variables manually effectively. However, even with these unoptimised variables, we were able to perform similarly to one of our best algorithms so far shows that there is potential for further optimisation. As this is effectively a parameter optimisation problem, a machine learning algorithm might suit the task. Again, it is out-of-scope for this research paper but provides a meaningful research opportunity.
