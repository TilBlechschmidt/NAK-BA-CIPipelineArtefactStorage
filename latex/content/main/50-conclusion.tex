\section{Conclusion}
    In this research paper, we have defined and analysed the process of decision-making regarding pipeline artefact storage with limited disk space available and explained the relevancy of the problem. Furthermore, we have designed and outlined a methodology to approach this challenge. In this process, we also looked at existing research from multiple different related areas like web caching or CPU memory paging and how this existing knowledge can be transferred into the realm of CI pipeline artefact storage.
    
    Using the methodology outlined previously, we designed a model to approximate reality and verify the relative performance of different decision-making algorithms. In building this model, many useful and relevant data sources like GitLab Webhooks, Access logs and disk contents have been identified and monitored. During this process, multiple gigabytes of data have been collected over of roughly eight weeks resulting in information about 1.868 merge requests, 3.771 CI pipelines, and 16.579 accesses to 1.339 distinct pipelines by developers and software testers. This data has then been processed and cleansed using a purpose-built pipeline. Based on the resulting data, a cross-platform reusable simulation has been constructed to evaluate different algorithms' performance. In this context, a number of abstractions and approximations have been made to simplify the simulation design and amount input data required.
    
    Based on the research done initially, and the data set collected, ten algorithms in three categories have been defined. During the evaluation, we have taken a closer look at some algorithms' inner workings and attempted to reason about their performance, advantages, and drawbacks where possible. To our surprise was one of the simplest algorithms (FIFO) able to compete with more complex algorithms that rely on domain-specific inputs. As a matter of fact, these domain-specific algorithms did have less headroom than initially expected. This brings up the question of whether collecting the data for more complex algorithms is worth it in the first place when applying these in a real-world environment. However, since the best performer across the board (STATUS) only requires trivial inputs which are expected to be available in most CI pipelines, it is recommended to do so.
    
    Aside from the simulation, we also looked at other methods and approaches to optimise the retention period and relevancy of the data stored. Additionally, we investigated ways to store the data at hand on disk more efficiently using linear memory slices with both compressed and uncompressed containers and what kind of gains are to be expected with these methods.

    \subsection{Applicability}
        First and foremost, it should be noted that the simulation and its surrounding methodology relies on the intentional abstraction that restricts the direct applicability of the results to real-life pipelines and artefacts. For this reason, particular caution should be taken when taking absolute numbers obtained in this paper (e.g. the number of misses for a given algorithm) and drawing any conclusions on the absolute real-life performance. Instead, this paper focused on analysing the relative performance between algorithms, and thus some approximations have been made which simplify the process of achieving this goal. However, as these have been carefully chosen and outlined in section \ref{sec:abstractions}, the relative performance of algorithms and other results obtained from the simulation, without regards to the original data source, are valid.
        
        However, when using the results, one may consider the data source used. Aspects like the data density, observation period, and data quality should be taken into consideration. Especially factors like, for example, holidays or project development lifecycles may influence the result. While we did our best during the data cleansing stage to keep these factors isolated, it is worth mentioning.
        
        Another relevant factor is that the team operated on, and we observed a system in which no significant access issues were present due to the sufficient availability of disk space. If this situation were different, it is far from guaranteed that the users behave in the same way. It is very well possible that previously mentioned but hardly used features like downloading, or flagging artefacts may be used more when the disk pressure is high.
        
        Regardless, it is expected that the results are transferable to teams with similar workflows and tooling. When transferring to different environments, caution may be applied.
        
        Despite all this, even in environments where the results are not directly transferable, or their applicability is questionable, it is still possible to re-evaluate the algorithms using the simulation. Special considerations went into the design of it to ensure the reproducibility of the results. It should be possible for anyone with access to the required data sources to reproduce the results in this paper based on their own teams' data.
    
    \subsection{Future research}
        Several research opportunities were encountered during the development of this paper and its associated data processing tools.
        
        Firstly, some potentially relevant data sources have been encountered. The first one is the teams' task management system which actively tracks all tasks' status and links to the associated version control system branches and CI pipelines. Collecting data from this source and integrating it into an algorithm could yield more in-depth results.
        
        Another potentially valuable data source is the version control system itself. Manual observations in the data revealed that a large number of pipelines have not been debugged at all, regardless of their state. Their results may have been superseded by another pipeline which is located further down the VCS commit chain. This information could be used to potentially more effectively decide whether or not an artefact is of value at all. Additionally, the code changes may be inspected to determine the components affected.
        
        An additional data source to look at is the status. While we did collect basic data about the return status of a pipeline, the full reports generated by them might contain additional insights (e.g. which test suite failed and what kind of error did it generate). However, it is expected that this requires a significant amount of work to classify the data.
        
        Regarding pipelines, it might be worthwhile to consider their different types and triggers. It is likely that a developer triggered pipeline is being used differently than a nightly run.\\
        \\
        Secondly, we mostly defined algorithms based on prior research and experience. However, it might be possible that a data-first approach may yield some high-performance algorithms. Analysing the data collected and detecting patterns or potential methods to develop improved algorithms provides an intriguing research opportunity.\\
        \\
        On the note of detecting patterns, it came up on multiple occasions that some algorithmic decisions, or parts of them, may lend themselves to the application of machine learning. An example brought up in section \ref{sec:machine-learning} was to split the decision-making process into two stages where the first decides whether a pipeline will be relevant at all and the second evaluates which algorithms to delete first as the disk pressure rises. In general, the development of new approaches and concepts for the decision-making process can prove to be a good starting ground for future research. As this paper provides a solid foundation to work with in the form of a fully automated simulation, it becomes relatively easy to integrate and evaluate new algorithms â€” allowing future research to focus solely on the algorithmic side of the problem at hand.
    
    \subsection{Data availability}\label{sec:data-availability}
        As some of the data sources used for this research paper contain confidential or personal information, not all raw data sets have been made available either in the paper or the accompanying source code repository. This allows the paper itself to be published without restrictions. Some data samples used in figures have been renamed for the same reason (e.g. figure \ref{fig:size-boxplot}), and names of interview partners and products have been omitted. While it is undoubtedly possible to anonymise both the raw data collected and the processed data used to generate figures and tables, this requires a significant amount of time and care. For this reason, the data will be retained by the author in its original form on a personal storage device and anonymised/distributed in an on-demand manner. If you require access to certain parts of the data for scientific reasons, please contact me directly via E-Mail at \href{mailto:til@blechschmidt.de}{til@blechschmidt.de}.
        
        All applications, scripts, and simulation components used in this paper are available at \href{https://github.com/TilBlechschmidt/NAK-BA-CIPipelineArtefactStorage}{https://github.com/TilBlechschmidt/NAK-BA-CIPipelineArtefactStorage}.