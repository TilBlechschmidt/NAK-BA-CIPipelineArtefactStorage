\section{Introduction}

    The concept of DevOps is becoming increasingly popular. Even though it appears that there is no clear definition of the term in peer-reviewed academic literature \cite{devops-definition}, an increasing number of developers and software testers vow for its importance. In the Stack Overflow Developer Survey 2020, close to 80\% of the participants considered it somewhat or extremely important \cite{devops-importance}. Similar numbers can be seen in other studies as well \cite{devops}. According to a study by SauceLabs and Dimensional Research in 2017, over 60\% of software testing operations make use of Continuous Integration (CI) pipelines\footnote{Which is considered part of the DevOps approach according to unscientific sources.} and more than 70\% use open-source browser automation frameworks in this context.
    
    With the advent of advanced testing tools that employ video recordings and extensive logging, a sharp increase in the amount of generated debugging data each day is expected. Making all this data available to developers and testers requires significant amounts of storage infrastructure. While storage itself got cheaper over the years \cite{hdd-prices}, keeping redundant and reliable storage systems online warrants even higher raw storage requirements. Nevertheless, looking beside storage volume alone, performance should also become a consideration. Scaling to infinity on spinning hard disk drives (HDD) has become increasingly unpopular with sales figures and volume for HDDs dropping ever since 2013 \cite{hdd-shipments-1} \cite{hdd-shipments-2}. In the meantime, solid-state drives (SSD) have started to take over the market \cite{ssd-sales}, very likely for their increased performance. However, this comes with a cost as SSDs are still considered more expensive and less dense than their spinning counterparts \cite{ssd-cost}.
    
    Considering these requirements on the storage infrastructure raises the question of how to most efficiently store the artefacts and results generated by a CI pipeline. This problem can be split into two categories: Storage density and management. The former concerns itself with the data's entropy on disk while the latter covers the topic of deciding what to store for how long. In this research paper, we will mainly focus on the second category and take a short peek at relevant topics for the former in section \ref{sec:further-considerations}.
    
    The main issue we will be taking on is the decision-making process involved with the mandatory deletion of CI pipeline artefacts. As the disk space is constrained, some artefacts will have to be purged eventually and deciding what to delete will be the primary focus. In general, this problem bears some resemblance to caching in the web or memory paging on the CPU. László Bélády devised a theoretical optimal algorithm for caching in relation to paging in 1966, best known as the Bélády Algorithm or Bélády's Min. His algorithm will delete the information that will not be needed for the longest time in the future \cite{belady-algo}. While this defines the perfect solution to the problem, it is near impossible to achieve it in practice due to the imperfect information basis on which decisions have to be made. This paper aims to find an answer to the following research question:
    
    \begin{displayquote}
        Which decision-making algorithm performs best when tasked to pick the next pipeline artefact to delete?
    \end{displayquote}
    
    While much research has gone into the evaluation and optimisation of algorithms in the realm of web caching \cite{web-caching-simulation}, it appears that very few papers have observed this general problem in the context of CI pipelines. As this environment poses its unique challenges, the results are expected to make a valuable contribution. Even though the research question focuses on finding the best algorithm, we will be putting a considerable amount of work into the process of crafting a reliable, methodical approach to evaluating algorithms in this context. Aside from finding the best-suited algorithm, a secondary goal will be to provide a testbed for future research. Inspired by the work done by H. Bahn et al. in 2003 \cite{web-caching-simulation}, we are going to build a fully automated, cross-platform simulation that provides the methodical ground-work allowing future papers to focus more closely on the development of algorithms themselves.

    This paper will be split into three methodical sections. Each section itself starts with a more detailed description of the methodical steps that will be taken to get closer to answering our research question. In section \ref{sec:modelling-reality}, we will be collecting data from a real-world CI pipeline. This data will then be cleansed and transformed. Furthermore, we will be building a simulation based on this to evaluate the performance of different algorithms. In the next section, section \ref{sec:evaluation}, a number of algorithms split into three categories will be outlined and evaluated for their performance using the previously created simulation. Finally, we will be looking at other options and considerations to improve the overall storage efficiency in section \ref{sec:further-considerations}.
