In the previous section, we collected real-world data from many different sources about the operations of a CI pipeline, the artefacts generated by it, and their usage by developers and software testers. In this section, we will be building on this foundation to build a simulation which approximates this real-world system during the time frame in which data was captured. While it is desirable to stay as close to reality as possible when modelling a system, some abstractions will be made and explained in the following section. Additionally, the modus operandi of the simulation will be described in detail in section \ref{sec:modus-operandi}.

\subsubsection{Abstractions \& Approximations}\label{sec:abstractions}
    % First abstraction: Level of detail - file vs. pipeline
    The first abstraction type to consider is at what detail the simulation will run whether we observe individual file-level accesses or generalise them as access to a specific pipeline. In general, the access log would theoretically provide sufficient information to simulate on a per-file level. However, it is not easily possible to discern between directory listings and file accesses. To get a better picture of what resources are accessed, we will run a query against the simulation input database. Table \ref{tbl:access-groups} shows the results of that query\footnote{Manual data cleansing has been performed to remove unique identifiers from paths and group similar paths together.}. The category "Automated requests" names requests which have not been caught during the data cleansing phase. "Other" groups anything that did not fit in any of the categories.

    \begin{Figure}
        \begin{center}
            \begin{tabular}{ l | r }
                Category & Count \\ \hline
                Test reports & 8471 \\
                Directory listing & 2673 \\ 
                Log files & 400 \\ 
                Automated request & 277 \\
                Other & 6
            \end{tabular}
        \end{center}
        \captionof{table}{Categorised access paths}
        \label{tbl:access-groups}
    \end{Figure}
    
    It is noteworthy that the majority of the requests were made to reports directly. This indicates that developers do not use the directory listing to navigate to the desired pipeline but instead use other facilities. Discussion with the team revealed that they automated embedding a direct link to the reports into their task management system. It should also be noted that 98,6\% of the test report requests were made to a total of 5 distinct paths\footnote{These paths are relative to the respective pipelines root directory.}. Directory listings are similar with 81,6\% of requests being covered by the top ten paths.
    
    These numbers indicate that there would be no significant gain from the added complexity of simulating on a file-level. For this reason and the lack of data on a pipeline content level (as with the size samples), we will treat a pipeline as an atomic entity with no further subdivisions. However, it should be noted that, while the majority of accesses were made to very few distinct paths, infrequently fewer files may not be deemed less essential. Some log files or data dumps might only become relevant during a critical failure and are otherwise ignored. Thus even if we were to run a file-level simulation, applying a simple statistical model regarding a files relevancy may not be a viable option.\\
    \\
    % Second abstraction: Time vs. event based stepping
    The second abstraction to consider is the step size of the simulation. Since all time-related input data points have an associated timestamp, it is possible to simulate discrete time steps. This approach would retain the relationship between events and their relative distance in the time dimension. However, it would also leave artefacts in the resulting data since less data is produced on weekends. Since our analysis is not time-related but instead focused on the relative performance of different algorithms, losing accuracy in the time dimension is considered a reasonable trade-off to eliminate the distortions caused by it. In order to abstract the time domain, the input data is used to generate simulation events. Each event corresponds to activity in the data set like the creation or completion of a pipeline, access to an artefact, or the merge of a feature branch. An overview of the different event types and their counts can be seen in table \ref{tbl:simulation-event-overview}. Data points at the capture time frame's boundaries cause the discrepancy between created and finished pipelines.

    \begin{Figure}
        \begin{center}
            \begin{tabular}{ l | r }
                Event type & Event count \\ \hline
                Artifact access & 16170 \\
                Pipeline created & 3771 \\
                Pipeline finished & 3759 \\
                Merge request change & 1868 \\
            \end{tabular}
        \end{center}
        \captionof{table}{Simulation events overview}
        \label{tbl:simulation-event-overview}
    \end{Figure}
    
    \pagebreak
    
    \colfig{image/graphs/event-time-lineplot}{Relation between time and simulation events}{fig:event-time}
    
    The relation between time and simulation events can be seen in figure \ref{fig:event-time}. It exhibits some irregularities. During the winter holidays between the 24th of December and the 5th of January, the number of incoming events is significantly reduced. Furthermore, weekends are visible around the 19th of December and the 9th of January.
    
    \colfig{image/graphs/access-event-lineplot}{Simulation event distribution}{fig:event-distribution}
    
    When abstracting the time component away and looking at simulation events in relation to the total count of events, we get the data depicted in figure \ref{fig:event-distribution}. Most of the variations have been eliminated. There is still a slight decrease in the access frequency at around 10.000 events which correlates with the Christmas holidays\footnote{This dip is still visible since time triggered pipelines were still executed during this period while most manual activity has stopped.}. However, this is less extreme for other event types, and the influx can approximately be considered constant. It should be noted that the data for completed pipelines occludes the data for created pipelines as they closely match.\\ % \todo{Maybe make this plot more readable somehow? Not sure if it actually gives us the power to say what we try to say. Distribution plot (percentage of 100\% for each type) might be more useful, albeit requiring interpolation of data points}
    \\
    % Third abstraction: Size sampling
    The third abstraction is one that is required due to the extent of the data available. During data capture, the sizes of the executed pipelines were not recorded for reasons described in section \ref{sec:data-source-sizes}. This requires an approximation of the total artefact size for the pipelines we will simulate. Based on the data at hand, we will be using a simple sampling approach on the data plotted in figure \ref{fig:size-boxplot} on page \pageref{fig:size-boxplot}. Since it is far from trivial to define a statistical distribution function which approximates the sizes, we will directly sample the collected data using a uniformly distributed pseudo-random number generator to determine the index to read. While this reduces accuracy, it makes the simulation more general-purpose. Since it is not our primary goal to achieve perfect simulation accuracy regarding the real-world data source, this is considered a reasonable trade-off. The random generator will be seeded once at the start of the simulation with a fixed seed to ensure reproducibility. Additionally, we will sample each pipeline's size in a fixed order during the initialisation stage and cache the result for the simulation duration. This ensures that pipeline sizes are always sampled in the same order even during parallelised batch operations.
    
    It should be noted that this abstraction will render any direct implications from the simulation result regarding real-world performance in the observed development team invalid as the data influx no longer equals the real-world influx. It would be viable to confirm the reliability of our approximation by evaluating a number of samples. However, as we are mainly interested in the algorithms' relative performance instead of the simulated algorithms' real-world performance in the observed team, this step is unnecessary. Thus, we can safely discard pipeline jobs for which we do not have sufficient size samples available and will ignore them during the simulation. To determine this cutoff value, we will be looking at the number of outliers in the boxplot. For every environment and test suite combination with more than 100 samples\footnote{This value is chosen based on the underlying data set and the convergence characteristics to ensure that we do not derive the cutoff value from sample sets that are in and of themselves too small.}, we will calculate the number of statistical outliers outside of $1,5 * IQR$ (or the number of data points outside the whiskers in the boxplot). The resulting values are listed in table \ref{tbl:size-outliers}, with the arithmetic average listed at the bottom. As a result, we will be discarding jobs with less than 30 size samples available (they are already excluded from figure \ref{fig:size-boxplot}). In total, 22,5\% (848) of the pipelines from the data set have been ignored for this reason.\label{sec:abstraction-size-sampling}
    % \todo{Source/example/reference for this kind of outlier based sampling}

    \begin{singlespace}
        \begin{Figure}
            \begin{center}
                \begin{tabular}{ l | r }
                    Samples & Outliers \\ \hline
                    735 & 72\\
                    729 & 134\\
                    436 & 81\\
                    344 & 114\\
                    342 & 26\\
                    305 & 53\\
                    279 & 35\\
                    242 & 14\\
                    240 & 24\\
                    229 & 16\\
                    211 & 13\\
                    199 & 21\\
                    192 & 36\\
                    192 & 15\\
                    178 & 18\\
                    177 & 19\\
                    177 & 17\\
                    175 & 7\\
                    174 & 8\\
                    165 & 3\\
                    156 & 5\\
                    152 & 19\\
                    149 & 7\\
                    148 & 34\\
                    138 & 1\\
                    120 & 8\\
                    119 & 0\\
                    100 & 14\\ \hline\hline
                    Average & ~29,07
                \end{tabular}
            \end{center}
            \captionof{table}{Size sample outlier counts}
            \label{tbl:size-outliers}
        \end{Figure}
    \end{singlespace}
    
    % Fourth approximation: Storage time (access before completion)
    The last approximation we will make is regarding the pipeline status. The collected data provides us with information about the creation and completion timestamp of a given pipeline. It also reveals that some accesses are made to pipelines before their completion. Due to this, we can no longer consider a pipeline "stored" solely based on its completion timestamp. However, using the creation date would strip the variable duration component of pipelines (some are more complex and take longer than others, e.g. developer vs release pipeline). For this reason, we will be using an approximation where a pipeline is considered "stored" from its creation but only use virtual disk space from its completion time. While this ignores the fact that pipelines can produce intermediate artefacts\footnote{An interview with one of the development operation engineers revealed that pipelines are usually taking a few minutes to copy artifacts and thus some are available before completion of the pipeline as a whole.}, we would not be able to simulate it in more detail based on the currently available data regardless.\\
    \\
    % Summary
    Overall, we make a number of abstractions and approximations which simplify the real-world project we are modelling. However, most of these only influence the absolute numbers' applicability from the simulation to the underlying development team. Since it is not our goal to simulate and estimate absolute numbers for the particular team but instead generate relative numbers for different algorithms, it is safe to make these simplifications. However, it should be noted that \emph{one may employ utmost caution when using absolute numbers from the simulation} to deduct any real-world performance.

% - File-level access log might not be possible
% 	- Dir-listings are indistinguishable
% 	- Compressed files are inaccessible
% 	- Give some statistics (maybe even a table) about accessed resources
% 		- Mostly html reports
% 		- Some dir-listings
% 		- Some random stuff
%       - SQL: SELECT COUNT(*) as c, file FROM AccessLog WHERE isAutomatic = 0 AND isIrrelevant = 0 AND file IS NOT NULL GROUP BY file ORDER BY c DESC;
% - Abstraction to per-pipeline model

\subsubsection{Modus operandi}\label{sec:modus-operandi}
    Based on the data we collected and the abstractions we made, we will now define a simulation that can be used to evaluate and compare different cleanup algorithms' performance. In this section, we will be focusing on the structure of the simulation, while the next section will focus on implementation-specific details.

    At its core, the simulation is a simple iterator. It iterates over all simulation events in the input database and processes them one by one\footnote{While these events could be derived from the cleaned data, it is more efficient to generate them once in the data source processors and re-use them for every simulation run.}. Events for which not enough data is available from all data sources (e.g. pipelines with low size samples or accesses to pipelines which the GitLab data sources has not captured) will be ignored as they would skew the results\footnote{Example: Accesses to a pipeline which does not exist will always fail and would thus influence the results in an undesirable way.}.
    
    \paragraph{Merge request} When a merge request event with the action "merge" is processed, the merged pipeline's identifier\footnote{The associated pipeline will be inferred based on the merge requests base branch.} will be stored and made available to algorithms. Other merge request events are currently ignored.
    
    \paragraph{Pipeline create} When a pipeline is created, its identifier and creation timestamp will be recorded in the list of "stored" pipelines (due to the abstraction described earlier). The creation timestamp is made available to algorithms.
    
    \paragraph{Pipeline finish} Once a pipeline is finished, its size will be sampled and added to the current virtual disk occupancy. It should be noted that this theoretically allows for a pipeline to be deleted before it is occupying disk space. However, this did not happen for the data set observed, so this approximation is acceptable.
    
    \paragraph{Access} When a request is encountered, the access counter will be increased. Additionally, it will be checked if the requested pipeline is currently stored. If that is not the case, the missed access counter will be incremented. Furthermore, the access timestamp will be recorded in association with the pipeline for cleanup algorithms to use.
    \\
    \\
    After each processed event, the simulation will check if its configured virtual disk size limit has been exceeded. If that is the case, it will delegate to a cleanup algorithm. The algorithm will then return a single pipeline identifier to delete, which will subsequently be removed from storage by the simulation\footnote{Removal entails a deletion of the pipeline identifier from the stored pipelines list and subtraction of its size from the disk occupancy variable.}. This process is repeated until the disk usage is below the limit. This causes the cleanup algorithm to run very frequently as only a small amount of disk space is "free" in regards to the limit, and the next incoming artefact will likely be larger than this free space. Running the cleanup algorithm frequently would incur significant I/O load if the system relies on on-disk metadata. For this reason, it is common to use a hysteresis where the cleanup algorithm operates with an upper limit and a separate cleanup target (e.g. cleanup when above 80GB, delete until below 60GB). However, as the simulation is operating solely in-memory, this is not a concern, and we will be omitting this mechanism\footnote{If this mechanism would be in-place the algorithms are expected to slightly lag behind the results we are seeing as they are simply deleting more, reducing the available disk space.}.
    
    Each time the simulation processes an event, the internal state will be captured and appended to a CSV formatted file. This includes the following metrics:
    
    \begin{itemize}
        \item Occupied storage
        \item Number of stored pipelines
        \item Number of deleted pipelines
        \item Access count
        \item Missed access count
    \end{itemize}
    
    This data can then be plotted using any tool that can read comma-separated values. In association with this paper, some Python-based plotting scripts are provided. They generate the figures shown in the following sections and are available in the accompanying source code repository.
    
    In total, a simulated total data influx of 13,7 TB and 25.568 events will be processed by the simulation based on the data collected.

\subsubsection{Implementation details}
    The data collectors, simulation, and plotting scripts have been implemented in cross-platform languages to ensure reproducibility and compatibility. All dependencies have been pinned to a specific patch version.
    
    Additionally, the simulation makes heavy use of internal caching of the data source in performance-critical areas (e.g. finding all past accesses to a pipeline). To further increase performance, all simulation code is implemented as asynchronous thread-safe code. This allows multi-threaded batch execution of the simulation.
    
    Regarding batch execution, there are a total of three modes available: One-shot execution of a single algorithm at a single virtual disk size, batch execution of several algorithms at a fixed virtual disk size, and size ramp batch execution which is similar to regular batch execution but runs for virtual disk sizes in a given range\footnote{The implementation takes a numerical range and raises it to the power of two to provide a reasonable distribution.}.
